# train.py
import torch
import torch.nn.functional as F
from torch_geometric.transforms import RandomLinkSplit
import os
import json
import pickle

# Imports locaux (Modularit√©)
from model import HGIB_Context_Model
from data_loader import load_and_process_data, build_graph

# ==========================================
# HYPERPARAM√àTRES (MLE-Ops Responsibility)
# ==========================================
HIDDEN_CHANNELS = 128
OUT_CHANNELS = 32
LEARNING_RATE = 0.0004
EPOCHS = 2000
BETA = 0.001
ARTIFACTS_DIR = "artifacts"

# Seuil de tol√©rance pour l'arr√™t (Delta)
EARLY_STOPPING_DELTA = 0.01  # Si val_loss > min_val_loss + 0.01, on arr√™te


def train_one_epoch(model, train_data, optimizer):
    model.train()
    optimizer.zero_grad()

    # Forward Pass
    pred, mu, logstd = model(train_data)
    target = train_data['user', 'visits', 'destination'].edge_label

    # 1. Reconstruction Loss (BCE)
    recons_loss = F.binary_cross_entropy_with_logits(pred, target)

    # 2. KL Divergence (Bottleneck)
    kl_loss = 0
    for key in mu.keys():
        kl_loss += -0.5 * torch.mean(
            torch.sum(1 + 2 * logstd[key] - mu[key] ** 2 - logstd[key].exp() ** 2, dim=1)
        )

    # Loss Totale
    loss = recons_loss + (BETA * kl_loss)

    loss.backward()
    optimizer.step()

    return loss.item(), recons_loss.item()


@torch.no_grad()
def evaluate(model, val_data):
    model.eval()
    pred, mu, logstd = model(val_data)
    target = val_data['user', 'visits', 'destination'].edge_label
    loss = F.binary_cross_entropy_with_logits(pred, target)
    return loss.item()


def main():
    # 1. Configuration
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"--- [MLE-Ops] Training Start on {device} ---")

    if not os.path.exists(ARTIFACTS_DIR):
        os.makedirs(ARTIFACTS_DIR)

    df, mappings = load_and_process_data()
    data = build_graph(df)

    # 3. Split Train/Val/Test
    transform = RandomLinkSplit(
        num_val=0.1,
        num_test=0.1,
        edge_types=[('user', 'visits', 'destination')],
        rev_edge_types=[('destination', 'rev_visits', 'user')],
        add_negative_train_samples=True
    )
    train_data, val_data, test_data = transform(data)

    train_data = train_data.to(device)
    val_data = val_data.to(device)

    # 4. Initialisation du Mod√®le
    num_acc = len(mappings['Accommodation type'])
    num_trans = len(mappings['Transportation type'])
    num_season = len(mappings['season'])
    num_users = data['user'].num_nodes
    num_dests = data['destination'].num_nodes

    model = HGIB_Context_Model(
        hidden_channels=HIDDEN_CHANNELS,
        out_channels=OUT_CHANNELS,
        metadata=data.metadata(),
        num_acc=num_acc,
        num_trans=num_trans,
        num_season=num_season,
        num_users=num_users,
        num_dests=num_dests
    ).to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)

    # 5. Boucle d'Entra√Ænement avec Early Stopping
    print(f"--- D√©but de l'entra√Ænement pour {EPOCHS} √©poques ---")
    print(f"--- Crit√®re d'arr√™t : Val Loss > Min Val Loss + {EARLY_STOPPING_DELTA} ---")

    min_val_loss = float('inf')
    best_model_state = None  # Pour garder le meilleur cerveau en m√©moire

    for epoch in range(1, EPOCHS + 1):
        train_loss, recons_loss = train_one_epoch(model, train_data, optimizer)
        val_loss = evaluate(model, val_data)

        # --- LOGIQUE EARLY STOPPING ---

        # Cas 1 : On trouve un nouveau record (le mod√®le s'am√©liore)
        if val_loss < min_val_loss:
            min_val_loss = val_loss
            best_model_state = model.state_dict()  # On sauvegarde cet √©tat pr√©cieux
            # On pourrait afficher un petit message de "Nouveau record" ici si on veut

        # Cas 2 : Le mod√®le diverge trop (Crit√®re d'arr√™t demand√©)
        elif val_loss > min_val_loss + EARLY_STOPPING_DELTA:
            print(f"\nüõë ARR√äT ANTICIP√â (Epoch {epoch})")
            print(
                f"   Raison : Val Loss ({val_loss:.4f}) a explos√© de +{val_loss - min_val_loss:.4f} par rapport au minimum ({min_val_loss:.4f}).")
            break

        if epoch % 10 == 0:
            print(
                f"Epoch {epoch:03d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} (Min: {min_val_loss:.4f})")

    print("--- Entra√Ænement termin√© ---")

    # 6. Sauvegarde des Artefacts
    print("--- Sauvegarde du MEILLEUR mod√®le et des mappings ---")

    # IMPORTANT : On recharge le meilleur √©tat (celui du minimum) avant de sauvegarder
    # Sinon on sauvegarderait le mod√®le "cass√©" qui vient d'exploser
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        print(f"‚úÖ Meilleur mod√®le restaur√© (Val Loss: {min_val_loss:.4f})")
    else:
        print("‚ö†Ô∏è Attention : Aucun meilleur mod√®le trouv√© (bizarre).")

    model_path = os.path.join(ARTIFACTS_DIR, "hgib_model.pth")
    torch.save(model.state_dict(), model_path)
    print(f"Mod√®le sauvegard√© : {model_path}")

    mappings_path = os.path.join(ARTIFACTS_DIR, "mappings.pkl")
    with open(mappings_path, 'wb') as f:
        pickle.dump(mappings, f)
    print(f"Mappings sauvegard√©s : {mappings_path}")

    config = {
        "hidden_channels": HIDDEN_CHANNELS,
        "out_channels": OUT_CHANNELS,
        "num_acc": num_acc,
        "num_trans": num_trans,
        "num_season": num_season
    }
    config_path = os.path.join(ARTIFACTS_DIR, "config.json")
    with open(config_path, 'w') as f:
        json.dump(config, f)
    print(f"Config sauvegard√©e : {config_path}")


if __name__ == "__main__":
    main()